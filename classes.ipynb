{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Data():\n",
    "    \n",
    "    def __init__(self,file_path,batch_size,zero_sample_fraction, zeroing = False):\n",
    "        \n",
    "        self.initial_df, self.input_df, self.mutant_list, self.labels, self.sample_weights = load_data(file_path)\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.zero_sample_fraction = zero_sample_fraction\n",
    "        \n",
    "        if zeroing:\n",
    "            self.labels[self.labels<2.7]=0\n",
    "            \n",
    "        self.x_train, self.x_val, self.idx_y_train, self.idx_y_val = train_test_split(self.input_df, \n",
    "                                                                                      [num for num in range(len(self.labels))], \n",
    "                                                                                      train_size=0.9)\n",
    "        self.y_train = self.labels[self.idx_y_train]\n",
    "        self.y_val = self.labels[self.idx_y_val]\n",
    "        self.sample_weights_train = self.sample_weights[self.idx_y_train]\n",
    "        self.sample_weights_val = self.sample_weights[self.idx_y_val]\n",
    "\n",
    "        self.all_inds = np.array([p for p in range(len(self.x_train))])\n",
    "        self.zero_inds = self.all_inds[self.y_train.flatten()==0]\n",
    "        self.nonzero_inds = self.all_inds[self.y_train.flatten()!=0]\n",
    "        \n",
    "    def plot_labels_distribution(self):\n",
    "        plt.figure(figsize=[10,8])\n",
    "        plt.hist(self.labels,bins=100,color='k',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResNet(object):\n",
    "    \n",
    "    @add_arg_scope\n",
    "    def ResNet_architecture(net, x, kernel_size, pool_size, weight_decay, keep_prob):\n",
    "\n",
    "        temp = bn(x, name='bn_first')\n",
    "        temp = conv(temp, 64, kernel_size=kernel_size, padding='SAME', \n",
    "                    kernel_initializer=xavier(), kernel_regularizer=l2_reg(weight_decay),\n",
    "                   name='conv_first')\n",
    "        temp = max_pool(inputs=temp, pool_size=pool_size, strides=2,padding='SAME')\n",
    "\n",
    "        for scale in range(net.num_scales):\n",
    "            with tf.variable_scope(\"scale%i\" % scale):\n",
    "                for rep in range(net.block_repeats):\n",
    "                    with tf.variable_scope(\"rep%i\" % rep):\n",
    "                        temp = residual_block(temp, kernel_size, weight_decay)\n",
    "\n",
    "                if scale < net.num_scales - 1:\n",
    "                    temp = conv(temp, 2 * x.get_shape().as_list()[-1],\n",
    "                             kernel_size=kernel_size, strides=2,\n",
    "                             padding='SAME', name='conv_downsample', kernel_regularizer=l2_reg(weight_decay))\n",
    "\n",
    "        temp = avg_pool(temp, pool_size=pool_size, strides=2)\n",
    "\n",
    "        temp = bn(temp, name='bn_last')\n",
    "        temp = tf.nn.relu(temp)\n",
    "\n",
    "        temp = tf.contrib.layers.flatten(temp)\n",
    "\n",
    "        temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "        temp = dense(temp,1000, activation=tf.nn.relu,name='dense_1', kernel_regularizer=l2_reg(weight_decay))\n",
    "        temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "        temp = dense(temp,1000, activation=tf.nn.relu,name='dense_2', kernel_regularizer=l2_reg(weight_decay))\n",
    "        temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "        temp = dense(temp,100, activation=tf.nn.relu,name='dense_3', kernel_regularizer=l2_reg(weight_decay))\n",
    "        temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "        temp = dense(temp,1, activation=tf.nn.relu,name='dense_4', kernel_regularizer=l2_reg(weight_decay))\n",
    "\n",
    "        return temp\n",
    "    \n",
    "    def __init__(self, input_data, num_scales, block_repeats, NN_name, mode,\n",
    "                kernel_size, pool_size, weight_decay, keep_prob, n_epoch):\n",
    "        \n",
    "        self.num_scales = num_scales\n",
    "        self.block_repeats = block_repeats\n",
    "        self.name = NN_name\n",
    "        self.mode = mode\n",
    "        self.n_epoch = n_epoch\n",
    "        \n",
    "        self.x_train_ph = tf.placeholder(dtype=tf.float32,shape=[None,238,21],name='train_X_ph')\n",
    "        self.y_train_ph = tf.placeholder(dtype=tf.float32,shape=[None,1],name='train_Y_ph')\n",
    "\n",
    "        self.x_val_ph = tf.placeholder(dtype=tf.float32,shape=[None,238,21],name='val_X_ph')\n",
    "        self.y_val_ph = tf.placeholder(dtype=tf.float32,shape=[None,1],name='val_Y_ph')\n",
    "\n",
    "        self.sample_weights_ph = tf.placeholder(dtype=tf.float32,shape=[None,1],name='sample_weights_ph')\n",
    "        \n",
    "        with tf.variable_scope(self.name):\n",
    "\n",
    "            with tf.device('/%s:0' % mode):\n",
    "                with arg_scope([conv]):\n",
    "                    with arg_scope([residual_block], keep_prob=keep_prob):\n",
    "                        with arg_scope([bn], training=True):\n",
    "                            with arg_scope([dense]):\n",
    "                                self.preds_train = ResNet_architecture(self, self.x_train_ph, \n",
    "                                                                   kernel_size, pool_size, \n",
    "                                                                   weight_decay, keep_prob)\n",
    "\n",
    "                with arg_scope([conv], reuse=True):\n",
    "                    with arg_scope([dense],reuse=True):\n",
    "                        with arg_scope([residual_block], keep_prob=1):\n",
    "                            with arg_scope([bn], training=False, reuse=True):\n",
    "                                self.preds_val = ResNet_architecture(self, self.x_val_ph, \n",
    "                                                                   kernel_size, pool_size, \n",
    "                                                                   weight_decay, keep_prob=1)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(self.y_train_ph,self.preds_train)\n",
    "        self.loss_weighted = tf.reduce_mean(tf.squared_difference(self.y_train_ph, self.preds_train)*self.sample_weights_ph)\n",
    "        self.loss_reg = [self.loss_weighted] + tf.reduce_sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "\n",
    "        self.val_loss = tf.losses.mean_squared_error(self.y_val_ph,self.preds_val)\n",
    "        \n",
    "        self.learning_rate = tf.placeholder(shape=[], dtype=tf.float32, name='learning_rate')\n",
    "        self.opt = tf.train.AdadeltaOptimizer(learning_rate=self.learning_rate)\n",
    "        self.train_step = self.opt.minimize(self.loss_reg)\n",
    "        self.train_step = tf.group(*([self.train_step] + tf.get_collection(tf.GraphKeys.UPDATE_OPS)))\n",
    "        \n",
    "        self.init = tf.global_variables_initializer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
