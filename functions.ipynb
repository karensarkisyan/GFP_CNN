{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tensorflow.contrib.framework.python.ops import add_arg_scope,arg_scope\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_mutant_sq(wt_sq, mutation_list):\n",
    "    '''Having a wt sequence and a list of mutations, this function combines the two into a full mutant sequence.'''\n",
    "    \n",
    "    mutant_sq = list(wt_sq)\n",
    "    for mutation in mutation_list:\n",
    "        assert mutation[0] == wt_sq[int(mutation[1:-1])+1]\n",
    "        #change mutated positions in the original sequence, to obtain the mutated sequence\n",
    "        mutant_sq[int(mutation[1:-1])+1] = mutation[-1]\n",
    "        \n",
    "    return ''.join(mutant_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_mutants_to_arrays(f):\n",
    "    \n",
    "    initial_df = pd.DataFrame.from_csv(f,sep='\\t',index_col=None)\n",
    "    \n",
    "    #extracting all the mutation combinations there are into a mutant_list \n",
    "    mutant_list = list(initial_df.aaMutations[1:])\n",
    "\n",
    "    #separating mutation combinations into lists\n",
    "    mutant_list = [[el[1:] for el in x.split(':')] for x in mutant_list]\n",
    "    \n",
    "    #recording the wild type sequence in order to be able to code the whole sq of mutants (and not only the mutations)\n",
    "    wt_sq = 'MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK'\n",
    "    \n",
    "    #start the list with a wild-type\n",
    "    mutant_sq_list = [wt_sq]\n",
    "\n",
    "    for mutant in mutant_list:\n",
    "        mutant_sq_list.append(make_mutant_sq(wt_sq, mutant))\n",
    "        \n",
    "    #creating the initial 0-filled carcas of unfolded versions of sqs, which will be further turned into binary matricies of shape 238 by 20 (aas)\n",
    "    unfolded_df = {}\n",
    "\n",
    "    for aa in set([item[-1] for sublist in mutant_list for item in sublist]):\n",
    "        unfolded_df[aa] = np.zeros((len(initial_df),len(wt_sq)))\n",
    "\n",
    "    #filling the binary matrices, corresponding to 20 different amino acids within the unfoded_df dict\n",
    "\n",
    "    for ind,mutant in enumerate(mutant_sq_list):\n",
    "        for pos,mut in enumerate(mutant):\n",
    "            unfolded_df[mut][ind, pos] = 1.\n",
    "        \n",
    "    #stacking all the amino acids into one np array\n",
    "    input_df = np.stack(unfolded_df.values(),axis=1)\n",
    "\n",
    "    #putting the channel info (amino acids) to the end\n",
    "    input_df = np.swapaxes(input_df,-1,-2)\n",
    "    \n",
    "    return initial_df, input_df, mutant_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(f, add_dist=False, dist_file='/nfs/scistore08/kondrgrp/eputints/Jupyter/mut_predictions/distances_to_chromophore.txt',\n",
    "             scale=False):\n",
    "    \n",
    "    initial_df, input_df, mutant_list = convert_mutants_to_arrays(f)\n",
    "    \n",
    "    labels = initial_df.medianBrightness\n",
    "    labels=labels.reshape(-1,1)\n",
    "\n",
    "    sample_weights = initial_df.uniqueBarcodes\n",
    "    sample_weights=sample_weights.reshape(-1,1)\n",
    "    \n",
    "    if add_dist == True:\n",
    "        \n",
    "        dists = pd.DataFrame.from_csv(dist_file,sep='\\t')\n",
    "        dists.columns=['Distance']\n",
    "        \n",
    "        #Adding info for positions that are not annotated (in the end of the protein and the chromophore)\n",
    "        for i in range(232,240):\n",
    "            dists.loc[i] = 22.088911\n",
    "\n",
    "        for i in [65,67]:\n",
    "            dists.loc[i] = 0.00\n",
    "        mutation_dist_list = [100.0]\n",
    "\n",
    "        for mutant in mutant_list:\n",
    "            temp_dist_list=[]\n",
    "            for mutation in mutant:\n",
    "                temp_dist_list.append(dists.Distance.ix[int(mutation[1:-1])+2])\n",
    "            mutation_dist_list.append(min(temp_dist_list))\n",
    "\n",
    "        mutation_dist_list=np.array(mutation_dist_list).reshape(-1,1)    \n",
    "        return initial_df, input_df, mutant_list, labels, sample_weights, mutation_dist_list\n",
    "    \n",
    "    else:\n",
    "        return initial_df, input_df, mutant_list, labels, sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting datasets to np (scipy) arrays\n",
    "\n",
    "# import scipy.sparse\n",
    "\n",
    "# for aa in unfolded_df:\n",
    "#     print aa\n",
    "#     sparse=scipy.sparse.csc_matrix(unfolded_df[aa])\n",
    "#     scipy.sparse.save_npz('arrays/'+aa+'.npz', sparse)\n",
    "    \n",
    "# np.save('brightness.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xavier = tf.contrib.layers.xavier_initializer\n",
    "l2_reg = tf.contrib.layers.l2_regularizer\n",
    "bn = add_arg_scope(tf.layers.batch_normalization)\n",
    "dense = add_arg_scope(tf.layers.dense)\n",
    "conv = add_arg_scope(tf.layers.conv1d)\n",
    "max_pool = add_arg_scope(tf.layers.max_pooling1d)\n",
    "avg_pool = add_arg_scope(tf.layers.average_pooling1d)\n",
    "dense = add_arg_scope(tf.layers.dense) \n",
    "\n",
    "@add_arg_scope\n",
    "def residual_block(a, kernel_size, weight_decay, keep_prob=1):\n",
    "\n",
    "    filters = a.get_shape().as_list()[-1]\n",
    "\n",
    "    b = bn(a,name='bn_first_rb')\n",
    "    b = tf.nn.relu(b)\n",
    "    b = tf.nn.dropout(b, keep_prob=0.9)\n",
    "    b = conv(b, filters, kernel_size=kernel_size, padding='SAME', \n",
    "             kernel_initializer=xavier(), kernel_regularizer=l2_reg(weight_decay),\n",
    "            name='conv_first_rb')\n",
    "\n",
    "    b = bn(b,name='bn_second_rb')\n",
    "    b = tf.nn.relu(b)\n",
    "    b = conv(b, filters, kernel_size=kernel_size, padding='SAME', \n",
    "             kernel_initializer=xavier(), kernel_regularizer=l2_reg(weight_decay),\n",
    "            name='conv_second_rb')\n",
    "\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@add_arg_scope\n",
    "def ResNet_architecture(net, x, kernel_size, pool_size, weight_decay, keep_prob):\n",
    "\n",
    "    temp = bn(x, name='bn_first')\n",
    "    temp = conv(temp, 64, kernel_size=kernel_size, padding='SAME', \n",
    "                kernel_initializer=xavier(), kernel_regularizer=l2_reg(weight_decay),\n",
    "               name='conv_first')\n",
    "    temp = max_pool(inputs=temp, pool_size=pool_size, strides=2,padding='SAME')\n",
    "\n",
    "    for scale in range(net.num_scales):\n",
    "        with tf.variable_scope(\"scale%i\" % scale):\n",
    "            for rep in range(net.block_repeats):\n",
    "                with tf.variable_scope(\"rep%i\" % rep):\n",
    "                    temp = residual_block(temp, kernel_size, weight_decay)\n",
    "\n",
    "            if scale < net.num_scales - 1:\n",
    "                temp = conv(temp, 2 * x.get_shape().as_list()[-1],\n",
    "                         kernel_size=kernel_size, strides=2,\n",
    "                         padding='SAME', name='conv_downsample', kernel_regularizer=l2_reg(weight_decay))\n",
    "\n",
    "    temp = avg_pool(temp, pool_size=pool_size, strides=2)\n",
    "\n",
    "    temp = bn(temp, name='bn_last')\n",
    "    temp = tf.nn.relu(temp)\n",
    "\n",
    "    temp = tf.contrib.layers.flatten(temp)\n",
    "\n",
    "    temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "    temp = dense(temp,1000, activation=tf.nn.relu,name='dense_1', kernel_regularizer=l2_reg(weight_decay))\n",
    "    temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "    temp = dense(temp,1000, activation=tf.nn.relu,name='dense_2', kernel_regularizer=l2_reg(weight_decay))\n",
    "    temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "    temp = dense(temp,100, activation=tf.nn.relu,name='dense_3', kernel_regularizer=l2_reg(weight_decay))\n",
    "    temp = tf.nn.dropout(temp, keep_prob=keep_prob)\n",
    "    temp = dense(temp,1, activation=tf.nn.relu,name='dense_4', kernel_regularizer=l2_reg(weight_decay))\n",
    "\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epoch_iterator(n, k):\n",
    "    \n",
    "    perm = np.random.permutation(n)\n",
    "    \n",
    "    for i in range(n / k):\n",
    "        yield perm[i * k:(i + 1) * k]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def epoch_iterator_balanced(n, batch_size, zero_share, zero_inds, nonzero_inds):\n",
    "    \n",
    "    for i in range(n / batch_size):\n",
    "        zero_elements = np.random.choice(zero_inds,int(batch_size*zero_share))\n",
    "        nonzero_elements = np.random.choice(nonzero_inds,int(batch_size*(1-zero_share)))\n",
    "        \n",
    "        export = np.append(zero_elements,nonzero_elements)\n",
    "        np.random.shuffle(export)\n",
    "        \n",
    "        yield export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reset_graph(seed=8):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN(nn_instance, input_data, patience, log_dir):\n",
    "    \n",
    "    print 'Initializing NN'\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "    \n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        sess.run(nn_instance.init)\n",
    "        train_mse_hist=[]\n",
    "        val_mse_hist=[]\n",
    "        print 'Epoch #\\t\\t\\tTrain MSE\\t\\tTest MSE'\n",
    "        \n",
    "        for epoch in range(nn_instance.n_epoch):\n",
    "\n",
    "            temp_train_mse = []\n",
    "            temp_val_mse = []\n",
    "\n",
    "            for idx in epoch_iterator_balanced(len(input_data.x_train), input_data.batch_size, \n",
    "                                               input_data.zero_sample_fraction, input_data.zero_inds, \n",
    "                                               input_data.nonzero_inds):\n",
    "\n",
    "                x_batch_train = input_data.x_train[idx]\n",
    "                y_batch_train = input_data.y_train[idx]\n",
    "                sample_weights_batch = input_data.sample_weights_train[idx]            \n",
    "\n",
    "                sess.run(nn_instance.train_step, {nn_instance.x_train_ph:x_batch_train,\n",
    "                                                  nn_instance.y_train_ph:y_batch_train,\n",
    "                                                  nn_instance.learning_rate:0.1 / (3 ** (epoch / 40)),\n",
    "                                                 nn_instance.sample_weights_ph:sample_weights_batch})\n",
    "\n",
    "                temp_train_mse.append(sess.run(nn_instance.loss, {nn_instance.x_train_ph:x_batch_train,\n",
    "                                                  nn_instance.y_train_ph:y_batch_train,\n",
    "                                                  nn_instance.learning_rate:0.1 / (3 ** (epoch / 40)),\n",
    "                                                 nn_instance.sample_weights_ph:sample_weights_batch}))\n",
    "\n",
    "            train_mse_hist.append(np.median(temp_train_mse))\n",
    "\n",
    "            if epoch%10==0:\n",
    "\n",
    "                for idx in epoch_iterator(len(input_data.x_val), input_data.batch_size):\n",
    "                    x_batch_val = input_data.x_val[idx]\n",
    "                    y_batch_val = input_data.y_val[idx]\n",
    "\n",
    "                    temp_val_mse.append(sess.run(nn_instance.val_loss, {nn_instance.x_val_ph:x_batch_val,\n",
    "                                                                   nn_instance.y_val_ph:y_batch_val}))\n",
    "\n",
    "                val_mse_hist.append(np.median(temp_val_mse))\n",
    "\n",
    "                saver.save(sess, os.path.join(log_dir, \"model.ckpt\"))\n",
    "\n",
    "                print '%d\\t\\t\\t%.2f\\t\\t\\t%.2f' % (epoch, np.median(temp_train_mse), np.median(temp_val_mse))\n",
    "\n",
    "            if epoch%patience==0 and epoch!=0:    \n",
    "                if min(val_mse_hist[-patience:]) < np.median(temp_val_mse):\n",
    "                    break\n",
    "                    \n",
    "    return train_mse_hist, val_mse_hist"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
